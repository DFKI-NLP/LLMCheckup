model_name,size,description,training_data
meta-llama/Llama-2-7b-chat-hf,7B,Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.,"Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data."
mistralai/Mistral-7B-v0.1,7B,"Mistral-7B-v0.1 is a transformer model, with the following architecture choices: Grouped-Query Attention, Sliding-Window Attention, Byte-fallback BPE tokenizer",Mistral was trained on datasets extracted from the open Web (Authors do not reveal the details)
petals-team/StableBeluga2,70B,Stable Beluga 2 is an auto-regressive language model fine-tuned on Llama2 70B.,Stable Beluga 2 is a Llama2 70B model finetuned on an Orca style Dataset
EleutherAI/pythia-2.8b-v0,2.8B,"The Pythia Scaling Suite is a collection of models developed to facilitate interpretability research. It contains two sets of eight models of sizes 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B.","The Pile is a 825GiB general-purpose dataset in English. It was created by EleutherAI specifically for training large language models. It contains texts from 22 diverse sources, roughly broken down into five categories: academic writing (e.g. arXiv), internet (e.g. CommonCrawl), prose (e.g. Project Gutenberg), dialogue (e.g. YouTube subtitles), and miscellaneous (e.g. GitHub, Enron Emails)"
tiiuae/falcon-rw-1b,1B,Falcon-RW-1B is a 1B parameters causal decoder-only model built by TII and trained on 350B tokens of RefinedWeb.,"RefinedWeb is a high-quality web dataset built by leveraging stringent filtering and large-scale deduplication. Falcon-RW-1B, trained on RefinedWeb only, matches or outperforms comparable models trained on curated data."
